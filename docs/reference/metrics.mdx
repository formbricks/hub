---
title: "Metrics and Observability"
description: "OTLP push metrics, distributed tracing, and correlation (X-Request-ID, trace_id in logs)."
---

## Overview

When enabled via environment variables, the Hub API exposes:

- **Metrics via OTLP push** (HTTP request metrics, event pipeline, webhook delivery). No scrape endpoint; metrics are pushed to the same OTLP endpoint as traces (e.g. every 60 seconds).
- **Distributed tracing** (HTTP and optional pipeline spans) when an OTLP or stdout exporter is configured.
- **Structured log correlation**: `trace_id`, `span_id`, and `request_id` are added to log records when the request passes through the Request ID middleware (and trace context when tracing is enabled).

When metrics or tracing are disabled (env vars empty or unset), a startup warning is logged and the corresponding providers are not created.

---

## Environment Variables

| Variable | Description | Enable behavior |
|----------|-------------|-----------------|
| `OTEL_METRICS_EXPORTER` | Set to `otlp` to enable metrics via OTLP push to the same endpoint as traces. Empty/unset = metrics disabled; a warning is logged at startup. | Metrics enabled |
| `OTEL_TRACES_EXPORTER` | Set to `otlp` or `stdout` to enable tracing. Empty/unset = tracing disabled; a warning is logged at startup. | Tracing enabled |
| `OTEL_EXPORTER_OTLP_ENDPOINT` | Optional. OTLP endpoint (e.g. `http://localhost:4318`) for both metrics and traces when using OTLP. The SDK reads this from the environment. | Used when metrics or tracing is enabled with OTLP |

See [Environment Variables](/reference/environment-variables#observability) for full details.

---

## Request correlation: X-Request-ID

The server supports the **`X-Request-ID`** header for correlating logs and traces per HTTP request:

- If the client sends `X-Request-ID`, the server propagates that value.
- If the client does not send it, the server generates a new UUID (v7) and uses it for the request.
- The same value is echoed in the response header `X-Request-ID`.
- When structured logging is used with the request context (e.g. `slog.InfoContext(r.Context(), ...)`), the default slog handler adds `request_id` to the log record when the Request ID middleware has run.

Use this header to correlate API calls with server logs and, when tracing is enabled, with trace IDs.

---

## Metrics reference

All Hub-specific metrics use the `hub_` prefix. Attribute values are bounded (allow-lists) to keep cardinality low.

### Counters

| Name | Labels | Description |
|------|--------|-------------|
| `hub_events_discarded_total` | `event_type` | Total number of events dropped because the event channel was full (backpressure). |
| `hub_webhook_jobs_enqueued_total` | `event_type` | Total number of webhook dispatch jobs enqueued to River. |
| `hub_webhook_provider_errors_total` | `reason` | Total provider errors (`list_failed`, `enqueue_failed`). |
| `hub_webhook_deliveries_total` | `event_type`, `status` | Total delivery outcomes by status: `success`, `retry`, `failed_final`. |
| `hub_webhook_disabled_total` | `reason` | Total webhooks disabled (`410_gone`, `max_attempts`). |
| `hub_webhook_dispatch_errors_total` | `reason` | Total dispatch errors (e.g. `get_webhook_failed`). |

### Gauges

| Name | Labels | Description |
|------|--------|-------------|
| `hub_event_channel_depth` | — | Current length of the event channel (saturation / backpressure before events are discarded). |
| `hub_river_queue_depth` | — | Current number of jobs in the River default queue in states `available`, `retryable`, or `scheduled` (waiting to be worked). |

### Histograms

| Name | Labels | Description |
|------|--------|-------------|
| `hub_message_publisher_fan_out_duration_seconds` | `event_type` | Time in seconds to process one event across all providers (fan-out). |
| `hub_webhook_delivery_duration_seconds` | `event_type`, `status` | Elapsed time per webhook delivery attempt in seconds (enables P99/SLO). Status: `success`, `retry`, `failed_final`. |

HTTP request count and duration are provided by the OpenTelemetry HTTP instrumentation (otelhttp) when metrics are enabled; they use standard semantic conventions (method, route pattern, status class).

---

## Tracing (spans)

When `OTEL_TRACES_EXPORTER` is set (e.g. `otlp` or `stdout`):

- **HTTP**: Each request gets a span (method, route, status) via otelhttp.
- **Pipeline** (optional): Spans such as `message_publisher.fan_out` and `webhook.deliver` can be added in the event and worker code for full trace continuity from API → event → job → HTTP POST.

Span names and attributes use bounded values (e.g. `event_type`, method, route, status) to avoid high cardinality. Tracing is disabled when `OTEL_TRACES_EXPORTER` is empty or unset.

---

## Trace context in logs

The default slog handler is wrapped so that **`request_id`** is added to log records when the context contains the request ID (set by the Request ID middleware). When tracing is enabled, **`trace_id`** and **`span_id`** (W3C string format) are also added when the context has an active span. Use `slog.InfoContext(ctx, ...)` (and other `*Context` variants) in request and worker paths so that the context carries the request ID and span.

---

## Recommended alerts and suggestions

These are suggested alert rules and descriptions for operators using Prometheus and Alertmanager (or similar). The application only exposes metrics; alerting is configured in your monitoring system.

| Alert / check | Severity | Suggestion |
|---------------|----------|------------|
| `hub_events_discarded_total` increases | Warning | Events are being dropped due to backpressure; consider scaling or increasing channel capacity. |
| HTTP 5xx rate or error ratio above threshold | Critical | Investigate server errors and dependencies. |
| `hub_webhook_deliveries_total{status="failed_final"}` rate high or success rate drops | Warning | Many webhooks are failing finally; check endpoints and disabled webhooks. |
| `hub_webhook_provider_errors_total` or `hub_webhook_dispatch_errors_total` increases | Warning | List/enqueue or dispatch failures; check DB and River. |
| `hub_event_channel_depth` or `hub_river_queue_depth` persistently high | Warning | Pipeline saturation; may lead to discarded events or slow delivery. |

Severity and thresholds (e.g. rate, ratio) should be tuned for your environment. Use these as a starting point and adapt to your runbooks.

---

## Best practices alignment

The current implementation is aligned with common Prometheus and OpenTelemetry practices:

| Practice | Status |
|----------|--------|
| **Naming** | Single-word prefix (`hub_`), snake_case, `_total` on counters, `_seconds` on duration histograms. |
| **Units** | Base units (seconds) with explicit `WithUnit("s")` on duration histograms. |
| **Cardinality** | Bounded attribute values via allow-lists in `internal/observability/names.go`; no user IDs or unbounded labels. |
| **Types** | Counters for cumulative counts, gauges for instantaneous depth, histograms for latency distributions. |
| **Instrumentation scope** | Single meter name `hub` in `app.go` for application metrics. |
| **Linting** | `promlinter` enabled in `.golangci.yml` to enforce Prometheus naming. |

**Histogram buckets:** Duration histograms use explicit second-based bucket boundaries (see `durationHistogramBounds` in `internal/observability/provider.go`) so that quantiles and SLOs (e.g. “95% under 300ms”) are accurate. The default OTel boundaries are millisecond-oriented and would misplace second-based observations.

**Optional improvements:** Consider `WithCardinalityLimit(2000)` on the MeterProvider if you need a hard cap on time series per instrument; add exemplars (trace ID linkage) when your Prometheus/backend supports them.

---

## Recommended follow-up metrics

Metrics not in the initial implementation but useful for future work:

- **Exemplars**: Link metric samples to trace IDs (requires exporter and backend support).
- **DB connection pool metrics**: Pool size, in-use, idle (if the driver exposes them).
- **Readiness/liveness**: `/ready` with DB ping (often grouped with observability).

---

## Integrating with SigNoz

[SigNoz](https://signoz.io/) is an open-source, OpenTelemetry-native observability platform (logs, metrics, traces). The Hub fits in with minimal configuration.

### Traces

Hub exports traces via **OTLP over HTTP**. Point the Hub at SigNoz’s OTLP HTTP endpoint (default port **4318**):

```bash
OTEL_TRACES_EXPORTER=otlp
OTEL_EXPORTER_OTLP_ENDPOINT=http://<signoz-otel-collector>:4318
```

Replace `<signoz-otel-collector>` with your SigNoz collector host (e.g. `localhost` for local, or the collector service name in Kubernetes/Docker). OTLP HTTP uses port **4318**; use **4317** only if your collector exposes OTLP gRPC and you switch the Hub to an OTLP gRPC exporter. No path is required; the client sends to `/v1/traces`.

After this, traces appear in SigNoz under the service name `hub-api` (from otelhttp).

### Metrics

Hub pushes **metrics via OTLP** (same collector endpoint as traces). Enable metrics and ensure the collector receives OTLP metrics:

1. Enable metrics on the Hub:

   ```bash
   OTEL_METRICS_EXPORTER=otlp
   OTEL_EXPORTER_OTLP_ENDPOINT=http://<signoz-otel-collector>:4318
   ```

2. Ensure your SigNoz/OpenTelemetry Collector is configured to receive OTLP metrics (same endpoint as traces; the client uses `/v1/metrics`). No Prometheus scrape is required.

SigNoz will then ingest both Hub application metrics (`hub_*`) and the HTTP request metrics produced by otelhttp.

### Summary

| Signal  | Hub setting | SigNoz side |
|--------|-------------|-------------|
| Traces | `OTEL_TRACES_EXPORTER=otlp`, `OTEL_EXPORTER_OTLP_ENDPOINT=http://<collector>:4318` | Collector receives OTLP HTTP on 4318 |
| Metrics | `OTEL_METRICS_EXPORTER=otlp`, same endpoint as traces | Collector receives OTLP metrics (push); no scrape |

For full SigNoz setup (collector, query service, UI), see [SigNoz docs](https://signoz.io/docs/) and [OpenTelemetry Collector guide](https://signoz.io/docs/instrumentation/opentelemetry-collector/).

---

## Related

- [Environment Variables](/reference/environment-variables) — `OTEL_*` and all configuration.
- [Webhooks and events architecture](/docs/webhooks-and-events-architecture) — Event flow and webhook delivery pipeline.
